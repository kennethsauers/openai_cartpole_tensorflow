{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from statistics import median, mean\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class enviroment():\n",
    "    def __init__(self):\n",
    "        self.LR = 1e-3\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.env.reset()\n",
    "        self.goal_steps = 500\n",
    "        self.score_requirement = 25\n",
    "        self.initial_games = 1000\n",
    "    def generate_data(self):\n",
    "        # [OBS, MOVES]\n",
    "        training_data = []\n",
    "        # all scores:\n",
    "        scores = []\n",
    "        # just the scores that met our threshold:\n",
    "        accepted_scores = []\n",
    "        # iterate through however many games we want:\n",
    "        for _ in range(self.initial_games):\n",
    "            score = 0\n",
    "            # moves specifically from this environment:\n",
    "            game_memory = []\n",
    "            # previous observation that we saw\n",
    "            prev_observation = []\n",
    "            # for each frame in 200\n",
    "            for _ in range(self.goal_steps):\n",
    "                # choose random action (0 or 1)\n",
    "                action = random.randrange(0,2)\n",
    "                # do it!\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # notice that the observation is returned FROM the action\n",
    "                # so we'll store the previous observation here, pairing\n",
    "                # the prev observation to the action we'll take.\n",
    "                if len(prev_observation) > 0 :\n",
    "                    game_memory.append([prev_observation, action])\n",
    "                prev_observation = observation\n",
    "                score+=reward\n",
    "                if done: break\n",
    "\n",
    "            # IF our score is higher than our threshold, we'd like to save\n",
    "            # every move we made\n",
    "            # NOTE the reinforcement methodology here. \n",
    "            # all we're doing is reinforcing the score, we're not trying \n",
    "            # to influence the machine in any way as to HOW that score is \n",
    "            # reached.\n",
    "            if score >= self.score_requirement:\n",
    "                accepted_scores.append(score)\n",
    "                for data in game_memory:\n",
    "                    # convert to one-hot (this is the output layer for our neural network)\n",
    "                    if data[1] == 1:\n",
    "                        output = [0,1]\n",
    "                    elif data[1] == 0:\n",
    "                        output = [1,0]\n",
    "\n",
    "                    # saving our training data\n",
    "                    training_data.append([data[0], output])\n",
    "\n",
    "            # reset env to play again\n",
    "            self.env.reset()\n",
    "            # save overall scores\n",
    "            scores.append(score)\n",
    "\n",
    "        # just in case you wanted to reference later\n",
    "        training_data_save = np.array(training_data)\n",
    "        np.save('saved.npy',training_data_save)\n",
    "\n",
    "        # some stats here, to further illustrate the neural network magic!\n",
    "        print('Average accepted score:',mean(accepted_scores))\n",
    "        print('Median score for accepted scores:',median(accepted_scores))\n",
    "        print(Counter(accepted_scores))\n",
    "\n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 11:30:49,178] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "model = enviroment()\n",
    "model.initial_games = 100000\n",
    "model.score_requirement = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 61.68511198945981\n",
      "Median score for accepted scores: 58.0\n",
      "Counter({50.0: 291, 51.0: 284, 52.0: 233, 54.0: 228, 53.0: 214, 55.0: 200, 56.0: 192, 57.0: 168, 58.0: 154, 60.0: 147, 59.0: 132, 62.0: 120, 61.0: 108, 65.0: 102, 63.0: 94, 64.0: 94, 66.0: 86, 68.0: 73, 67.0: 72, 71.0: 68, 69.0: 67, 70.0: 63, 72.0: 49, 73.0: 42, 74.0: 41, 80.0: 38, 75.0: 33, 76.0: 31, 77.0: 29, 78.0: 26, 79.0: 25, 83.0: 23, 86.0: 23, 82.0: 22, 84.0: 20, 89.0: 19, 88.0: 17, 85.0: 14, 81.0: 13, 91.0: 13, 98.0: 13, 87.0: 12, 90.0: 10, 95.0: 9, 94.0: 8, 92.0: 7, 96.0: 7, 93.0: 6, 106.0: 6, 110.0: 5, 97.0: 4, 101.0: 4, 103.0: 4, 104.0: 4, 105.0: 3, 119.0: 3, 102.0: 2, 111.0: 2, 112.0: 2, 120.0: 2, 136.0: 1, 137.0: 1, 138.0: 1, 142.0: 1, 145.0: 1, 100.0: 1, 107.0: 1, 108.0: 1, 113.0: 1, 114.0: 1, 115.0: 1, 118.0: 1, 121.0: 1, 122.0: 1})\n"
     ]
    }
   ],
   "source": [
    "x = model.generate_data()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
