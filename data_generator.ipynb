{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from statistics import median, mean\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class enviroment():\n",
    "    def __init__(self):\n",
    "        self.LR = 1e-3\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.env.reset()\n",
    "        self.goal_steps = 500\n",
    "        self.score_requirement = 25\n",
    "        self.initial_games = 1000\n",
    "    def generate_data(self):\n",
    "        # [OBS, MOVES]\n",
    "        training_data = []\n",
    "        # all scores:\n",
    "        scores = []\n",
    "        # just the scores that met our threshold:\n",
    "        accepted_scores = []\n",
    "        # iterate through however many games we want:\n",
    "        for _ in range(self.initial_games):\n",
    "            score = 0\n",
    "            # moves specifically from this environment:\n",
    "            game_memory = []\n",
    "            # previous observation that we saw\n",
    "            prev_observation = []\n",
    "            # for each frame in 200\n",
    "            for _ in range(self.goal_steps):\n",
    "                # choose random action (0 or 1)\n",
    "                action = random.randrange(0,2)\n",
    "                # do it!\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # notice that the observation is returned FROM the action\n",
    "                # so we'll store the previous observation here, pairing\n",
    "                # the prev observation to the action we'll take.\n",
    "                if len(prev_observation) > 0 :\n",
    "                    game_memory.append([prev_observation, action])\n",
    "                prev_observation = observation\n",
    "                score+=reward\n",
    "                if done: break\n",
    "\n",
    "            # IF our score is higher than our threshold, we'd like to save\n",
    "            # every move we made\n",
    "            # NOTE the reinforcement methodology here. \n",
    "            # all we're doing is reinforcing the score, we're not trying \n",
    "            # to influence the machine in any way as to HOW that score is \n",
    "            # reached.\n",
    "            if score >= self.score_requirement:\n",
    "                accepted_scores.append(score)\n",
    "                for data in game_memory:\n",
    "                    # convert to one-hot (this is the output layer for our neural network)\n",
    "                    if data[1] == 1:\n",
    "                        output = [0,1]\n",
    "                    elif data[1] == 0:\n",
    "                        output = [1,0]\n",
    "\n",
    "                    # saving our training data\n",
    "                    training_data.append([data[0], output])\n",
    "\n",
    "            # reset env to play again\n",
    "            self.env.reset()\n",
    "            # save overall scores\n",
    "            scores.append(score)\n",
    "\n",
    "        # just in case you wanted to reference later\n",
    "        training_data_save = np.array(training_data)\n",
    "        np.save('saved.npy',training_data_save)\n",
    "\n",
    "        # some stats here, to further illustrate the neural network magic!\n",
    "        print('Average accepted score:',mean(accepted_scores))\n",
    "        print('Median score for accepted scores:',median(accepted_scores))\n",
    "        print(Counter(accepted_scores))\n",
    "\n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-28 15:32:27,047] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 61.55036456926816\n",
      "Median score for accepted scores: 58.0\n",
      "Counter({51.0: 288, 50.0: 270, 52.0: 261, 54.0: 232, 53.0: 204, 55.0: 196, 56.0: 173, 57.0: 160, 59.0: 152, 61.0: 142, 58.0: 141, 60.0: 132, 63.0: 113, 64.0: 111, 62.0: 102, 65.0: 101, 66.0: 69, 67.0: 65, 72.0: 61, 68.0: 53, 69.0: 53, 71.0: 52, 74.0: 46, 70.0: 43, 73.0: 41, 80.0: 30, 75.0: 29, 78.0: 29, 81.0: 27, 77.0: 25, 79.0: 23, 76.0: 21, 88.0: 18, 82.0: 16, 86.0: 16, 84.0: 15, 87.0: 13, 93.0: 13, 83.0: 12, 85.0: 12, 89.0: 12, 94.0: 11, 90.0: 9, 91.0: 8, 92.0: 8, 95.0: 8, 96.0: 6, 99.0: 6, 102.0: 6, 97.0: 5, 100.0: 5, 103.0: 5, 111.0: 5, 98.0: 4, 101.0: 4, 105.0: 3, 106.0: 3, 107.0: 3, 109.0: 3, 119.0: 3, 104.0: 2, 112.0: 2, 113.0: 2, 114.0: 2, 115.0: 2, 116.0: 2, 117.0: 2, 118.0: 2, 131.0: 1, 134.0: 1, 137.0: 1, 140.0: 1, 108.0: 1, 110.0: 1, 121.0: 1, 124.0: 1, 125.0: 1, 127.0: 1})\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = enviroment()\n",
    "    model.initial_games = 100000\n",
    "    model.score_requirement = 50\n",
    "    x = model.generate_data()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
