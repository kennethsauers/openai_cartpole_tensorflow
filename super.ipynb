{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from statistics import median, mean\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self):\n",
    "        self.LR = 1e-3\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.model = self.neural_network_model()\n",
    "        self.env.reset()\n",
    "        self.goal_steps = 500\n",
    "        self.score_requirement = 50\n",
    "        self.initial_games = 100000\n",
    "        \n",
    "    \n",
    "    def neural_network_model(self):\n",
    "        keep = 0.8\n",
    "        input_size = 4\n",
    "        LR = 1e-3\n",
    "        network = input_data(shape=[None, input_size, 1], name='input')\n",
    "        network = fully_connected(network, 128, activation='relu', name = 'hidden_1')\n",
    "        network = dropout(network,keep)\n",
    "        network = fully_connected(network, 256, activation='relu', name = 'hidden_2')\n",
    "        network = dropout(network,keep)\n",
    "        network = fully_connected(network, 512, activation='relu', name = 'hidden_3')\n",
    "        network = dropout(network,keep)\n",
    "        network = fully_connected(network, 256, activation='relu', name = 'hidden_4')\n",
    "        network = dropout(network,keep)\n",
    "        network = fully_connected(network, 128, activation='relu', name = 'hidden_5')\n",
    "        network = dropout(network,keep)\n",
    "        network = fully_connected(network, 2, activation='softmax', name = 'softmax')\n",
    "        network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "        model = tflearn.DNN(network, tensorboard_verbose=3)\n",
    "\n",
    "        return model\n",
    "        \n",
    "    def generate_data(self):\n",
    "        # [OBS, MOVES]\n",
    "        training_data = []\n",
    "        # all scores:\n",
    "        scores = []\n",
    "        # just the scores that met our threshold:\n",
    "        accepted_scores = []\n",
    "        # iterate through however many games we want:\n",
    "        for _ in range(self.initial_games):\n",
    "            score = 0\n",
    "            # moves specifically from this environment:\n",
    "            game_memory = []\n",
    "            # previous observation that we saw\n",
    "            prev_observation = []\n",
    "            # for each frame in 200\n",
    "            for _ in range(self.goal_steps):\n",
    "                # choose random action (0 or 1)\n",
    "                action = random.randrange(0,2)\n",
    "                # do it!\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "                # notice that the observation is returned FROM the action\n",
    "                # so we'll store the previous observation here, pairing\n",
    "                # the prev observation to the action we'll take.\n",
    "                if len(prev_observation) > 0 :\n",
    "                    game_memory.append([prev_observation, action])\n",
    "                prev_observation = observation\n",
    "                score+=reward\n",
    "                if done: break\n",
    "\n",
    "            # IF our score is higher than our threshold, we'd like to save\n",
    "            # every move we made\n",
    "            # NOTE the reinforcement methodology here. \n",
    "            # all we're doing is reinforcing the score, we're not trying \n",
    "            # to influence the machine in any way as to HOW that score is \n",
    "            # reached.\n",
    "            if score >= self.score_requirement:\n",
    "                accepted_scores.append(score)\n",
    "                for data in game_memory:\n",
    "                    # convert to one-hot (this is the output layer for our neural network)\n",
    "                    if data[1] == 1:\n",
    "                        output = [0,1]\n",
    "                    elif data[1] == 0:\n",
    "                        output = [1,0]\n",
    "\n",
    "                    # saving our training data\n",
    "                    training_data.append([data[0], output])\n",
    "\n",
    "            # reset env to play again\n",
    "            self.env.reset()\n",
    "            # save overall scores\n",
    "            scores.append(score)\n",
    "\n",
    "        # just in case you wanted to reference later\n",
    "        training_data_save = np.array(training_data)\n",
    "        np.save('saved.npy',training_data_save)\n",
    "\n",
    "        # some stats here, to further illustrate the neural network magic!\n",
    "        print('Average accepted score:',mean(accepted_scores))\n",
    "        print('Median score for accepted scores:',median(accepted_scores))\n",
    "        print(Counter(accepted_scores))\n",
    "\n",
    "        return training_data\n",
    "    \n",
    "    def train_model(self, model=False):\n",
    "        training_data = np.load('saved.npy')\n",
    "        X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "        y = [i[1] for i in training_data]\n",
    "        keep = 0.8\n",
    "        self.model.fit({'input': X}, {'targets': y}, n_epoch=1, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
    "        self.model.save('model_save/test')\n",
    "        return model\n",
    "    \n",
    "    def play(self, render = False, num = 100):\n",
    "        scores = []\n",
    "        choices = []\n",
    "        for each_game in range(num):\n",
    "            score = 0\n",
    "            game_memory = []\n",
    "            prev_obs = []\n",
    "            self.env.reset()\n",
    "            for _ in range(self.goal_steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                if len(prev_obs)==0:\n",
    "                    action = random.randrange(0,2)\n",
    "                else:\n",
    "                    action = np.argmax(self.model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
    "\n",
    "                choices.append(action)\n",
    "\n",
    "                new_observation, reward, done, info = self.env.step(action)\n",
    "                prev_obs = new_observation\n",
    "                game_memory.append([new_observation, action])\n",
    "                score+=reward\n",
    "                if done: break\n",
    "            scores.append(score)\n",
    "\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "    \n",
    "    def run(self):\n",
    "        self.generate_data()\n",
    "        self.train_model()\n",
    "        self.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3509  | total loss: \u001b[1m\u001b[32m0.66904\u001b[0m\u001b[0m | time: 112.935s\n",
      "| Adam | epoch: 001 | loss: 0.66904 - acc: 0.6032 -- iter: 224576/224604\n",
      "Training Step: 3510  | total loss: \u001b[1m\u001b[32m0.66694\u001b[0m\u001b[0m | time: 112.965s\n",
      "| Adam | epoch: 001 | loss: 0.66694 - acc: 0.5991 -- iter: 224604/224604\n",
      "--\n",
      "INFO:tensorflow:/home/hedonist/Documents/openai_cartpole_tensorflow/model_save/test is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-30 12:06:41,633] /home/hedonist/Documents/openai_cartpole_tensorflow/model_save/test is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 195.28\n",
      "choice 1:0.5015362556329374  choice 0:0.4984637443670627\n"
     ]
    }
   ],
   "source": [
    "more = agent()\n",
    "more.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 194.93\n",
      "choice 1:0.5016159647052788  choice 0:0.4983840352947212\n"
     ]
    }
   ],
   "source": [
    "more.play()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
